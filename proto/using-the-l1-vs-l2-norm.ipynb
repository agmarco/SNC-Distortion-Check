{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The L1 vs L2 Norm\n",
    "\n",
    "We have formulated our point cloud registration algorithm as an optimization problem that minimizes the following objective function:\n",
    "\n",
    "$$\n",
    "f_1(x, y, z, \\theta, \\phi, \\xi) =\n",
    "\\sum_B g(\\left|\\mathbf{b}\\right|)\n",
    "\\;\n",
    "(\\left|\\mathbf{b} - \\mathbf{a}_{\\textrm{S},\\textrm{min}}\\right|/\\rho(|\\mathbf{b}|) - 1)\n",
    "\\;\n",
    "u(\\rho(|\\mathbf{b}|) - \\left|\\mathbf{b} - \\mathbf{a}_{\\textrm{S},\\textrm{min}}\\right|)\n",
    "$$\n",
    "\n",
    "The objective function $f_1$ uses a weighted sum of the distances between pairs of matching points.  We will refer to this approach as \"using the L1 norm\".\n",
    "\n",
    "Alternatively, we could have optimized over the square-root of the sum of the distances-squared between pairs of matching points.  This is often called the RMS error, or the L2 norm.  If we had, we would be minimizing an objective function with a form similar to this:\n",
    "\n",
    "$$\n",
    "f_2(x, y, z, \\theta, \\phi, \\xi) =\n",
    "\\left[ \\sum_B g(\\left|\\mathbf{b}\\right|)\n",
    "\\;\n",
    "\\left(\\mathbf{b} - \\mathbf{a}_{\\textrm{S},\\textrm{min}}\\right)^2\n",
    "\\;\n",
    "u(\\rho(|\\mathbf{b}|) - \\left|\\mathbf{b} - \\mathbf{a}_{\\textrm{S},\\textrm{min}}\\right|)\n",
    "\\right]^{1/2}\n",
    "$$\n",
    "\n",
    "For some problems it may be appropriate to minimize the L1 norm, and for other problems it may be appropriate to minimize the L2 norm.  Deciding which is more correct completely depends on the context and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Reading\n",
    "\n",
    "Here is the abstract of a paper about this:\n",
    "\n",
    "> L2-norm,  also  known  as  the  least  squares  method  was  widely  used  in  the  adjustment  calculus.  The weaknesses of the least squares method were the effect of gross measurements error on the solution and the disturbance and absorbance of gross error on the solution. L1–norm, also known as the least absolute values method, was affected by almost none or very little from gross error. Therefore L1-norm method,  used  for parameters estimation in some  special  case, has  been  successfully used for outlier measurements  detection.  In  this  study,  the  L1  and  L2-norm  adjustment  methods  have  been  taken relatively  to  each  other’s  advantages  and  disadvantages  and  the  numerical  application  of  the  two-dimensional  similarity  coordinate  transformation  were  made  and  the  results  of  both  methods  are discussed.\n",
    "\n",
    "[Full article here](https://www.researchgate.net/publication/228416411_The_comparison_of_L1_and_L2-norm_minimization_methods)---it is not particularly well written FYI.\n",
    "\n",
    "[This blog post](http://www.chioka.in/differences-between-the-l1-norm-and-the-l2-norm-least-absolute-deviations-and-least-squares/) also has some interesting discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Results\n",
    "\n",
    "In our note book titled \"The Ideal Form of G\", it was seen that our registration algorithm found the precisely correct registration vector, regardless of how many points in the data set we considered.  This was surprising because one would expect that all of the randomness in the peripheral points should affect the registration vector if they were included.\n",
    "\n",
    "After further investigation, we believe that these results, although surprising, are correct.  We do not believe they are due to an error in our formulation of the problem or in our implementation of the solution.\n",
    "\n",
    "We believe our registration algorithm was able to find the precise registration vector because our objective function, $f$, uses the L1 norm.\n",
    "\n",
    "In particular, there were enough points without any distortion in the center of the volume such that the minimum of $f_1$ was still at the exact registration, despite the distorted points.  If the distortions in the ouside points had been larger, we believe that we would see the registration result change.\n",
    "\n",
    "This explains why our third example, which had slight amounts of distortion even in the central points, varied slightly as the $g$-cutoff increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of L1 Norm's \"Robustness\"\n",
    "\n",
    "\n",
    "In order to further verify our result, we reproduced it in a smaller, easier to visualize dataset.\n",
    "\n",
    "Here is an example 2D case where we can see a similar phenonmena.  Note that the objective function we are optimizing is slightly different, however we believe the results to be analgous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import scipy.optimize\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_f1(points):\n",
    "    def f(inputs):\n",
    "        m, b = inputs\n",
    "        return sum([abs(m*x + b - y) for x, y in points])\n",
    "    return f\n",
    "\n",
    "\n",
    "def build_f2(points):\n",
    "    def f(inputs):\n",
    "        m, b = inputs\n",
    "        return sqrt(sum([(m*x + b - y)**2 for x, y in points]))\n",
    "    return f\n",
    "\n",
    "def plot_f1_and_f2(points):\n",
    "    x = np.array([x for x, y in points])\n",
    "    y = np.array([y for x, y in points])\n",
    "    \n",
    "    plt.scatter(x, y, color='r', label='Raw Data')\n",
    "    \n",
    "    result = scipy.optimize.minimize(build_f1(points), x0=[0, 1])\n",
    "    m1, b1 = result.x\n",
    "    plt.plot(x, m1*x + b1, 'b-', label='L1 Norm')\n",
    "    \n",
    "    result = scipy.optimize.minimize(build_f2(points), x0=[0, 1])\n",
    "    m2, b2 = result.x\n",
    "    plt.plot(x, m2*x + b2, 'g-', label='L2 Norm')\n",
    "    plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_f1_and_f2([(-4, -3), (-3, -5), (-2, -2), (-1, -1), (0, 0), (1, 1), (2, 2), (3, 6), (4, 3)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the line derived from optimizing the L2 norm is tilted because of the \"distorted\" points in the periphery, while the line derived from the L1 norm is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 vs L1 for Our Registration Problem\n",
    "\n",
    "We believe that the comment in the abstract of *Potter 2000*:\n",
    "\n",
    "> Registration techniques based on matching of landmark points located far from the magnet isocenter are especially prone to MR distortions.\n",
    "\n",
    "is made based on conclusions drawn from various groups that used the L2 norm when performing their registration.\n",
    "\n",
    "> The evaluation of the registration accuracy with land- mark techniques is a topic of some controversy. A commonly used definition of the registration error in landmark registra- tion techniques is the root-mean-square (RMS) distance between corresponding landmarks. The error is then estimat- ed only at the landmark locations, and is assumed uniform throughout the registered volume.\n",
    "\n",
    "As far as we can tell, all of the papers they reference use the L2 norm as well.\n",
    "\n",
    "**We believe that for this registration problem, the L1 norm is much more appropriate than the L2 because we know that there is a certain bounded amount of distortion in points that are far from the origin.**\n",
    "\n",
    "The L2 norm is innappropriate for this problem, because the L2 norm weights outliers much more heavily than the L2 norm.  We know that the external points have significant distortion, we certainly do not want to weight them more than the central points.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Even Use the Outside Points?\n",
    "\n",
    "Really there are two questions:\n",
    "\n",
    "1. Should we use the L1 or L2 norm?\n",
    "2. How many points should we use?\n",
    "\n",
    "As mentioned, we believe we should use the L1 norm.  We also believe that we should weight the points in the middle higher by using a $g$ that tapers off as we leave the origin.  We also beleive that we should ignore points that are too far away from their \"expected position\" by using a $\\rho$ that grows larger as we move further away from the origin.\n",
    "\n",
    "By combining the effects of $g$, $\\rho$, and the L1-norm, we believe will be able to robustly find the proper registration, despite there being distortion in the points that are further from the center.\n",
    "\n",
    "However, one may ask, why even bother taking the risk--why not just use the L1-norm on just the central points?\n",
    "\n",
    "We are less certain of this conclusion, however we suspect that by taking into account the full set of points, our algorithm will be\n",
    "\n",
    "- more robust to errors and noise in the point locations near the isocenter\n",
    "- more robust to false positives near the isocenter\n",
    "- more robust to missing points (false negatives) near the isocenter\n",
    "- more robust to larger displacements of the phantom--i.e. they could be off by a full 1.5 cm and our algorithm should still be able to find the proper central location.\n",
    "\n",
    "All of that being said, we suspect that none of these items will be a problem in practice, and as a result, it will likely be more or less equivalent to use just the central points or the entire set of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cirs]",
   "language": "python",
   "name": "conda-env-cirs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
